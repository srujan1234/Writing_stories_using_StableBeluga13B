{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n**Title:** Text Generation with StableBeluga-13B Model\n\n\n\nThis Kaggle notebook demonstrates the exciting capabilities of the StableBeluga-13B language model from [stabilityai](https://huggingface.co/stabilityai/StableBeluga-13B). The code showcases how to use this powerful model for text generation and applies it to create short stories based on an inspirational quotes dataset.\n\n\n\n1. **Model Loading:** The notebook guides you through loading the StableBeluga-13B model using the popular Transformers library by Hugging Face.\n\n2. **Dataset Preparation:** It utilizes a dataset of cleaned quotes (CSV format) to inspire the short story generation process.\n\n3. **Story Generation Function:** A function named `generate_story` is defined to take a quote as input, tokenize it, and generate a short story using the StableBeluga-13B model.\n\n4. **Customizable Story Generation:** The code allows for easy customization of the story generation process, including parameters like maximum length, number of beams, temperature, and more.\n\n**How to Use:**\n\n1. Load the required libraries and install the Transformers library (`pip install transformers`).\n2. Load the dataset of cleaned quotes (e.g., \"cleaned_quotes.csv\").\n3. Run the provided code snippets to load the StableBeluga-13B model and define the story generation function.\n4. Generate inspiring short stories based on the provided quotes.\n\n","metadata":{}},{"cell_type":"code","source":"pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-11T23:12:44.302205Z","iopub.execute_input":"2023-11-11T23:12:44.302695Z","iopub.status.idle":"2023-11-11T23:12:58.910158Z","shell.execute_reply.started":"2023-11-11T23:12:44.302653Z","shell.execute_reply":"2023-11-11T23:12:58.909010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"stabilityai/StableBeluga-13B\")","metadata":{"execution":{"iopub.status.busy":"2023-11-11T23:12:58.911935Z","iopub.execute_input":"2023-11-11T23:12:58.912209Z","iopub.status.idle":"2023-11-11T23:17:01.033204Z","shell.execute_reply.started":"2023-11-11T23:12:58.912184Z","shell.execute_reply":"2023-11-11T23:17:01.032274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"stabilityai/StableBeluga-13B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"stabilityai/StableBeluga-13B\")","metadata":{"execution":{"iopub.status.busy":"2023-11-11T23:17:01.034395Z","iopub.execute_input":"2023-11-11T23:17:01.034921Z","iopub.status.idle":"2023-11-11T23:18:58.370469Z","shell.execute_reply.started":"2023-11-11T23:17:01.034894Z","shell.execute_reply":"2023-11-11T23:18:58.369679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport random\n\n# Load the dataset\ndataset_path = \"/kaggle/input/cleaneddd/cleaned_quotes.csv\"\ndf = pd.read_csv(dataset_path)\n\n# Define a function to generate a short story\ndef generate_story(quote):\n    # You can customize the story generation logic here.\n    # For this example, we'll generate a random story based on the quote.\n    story = f\"Write a short story with minimum 500 words based on the quote:\\n'{quote}'\\n\"\n    \n    return story\n\n# Generate stories for the first 10 rows\nfor index, row in df.head(1).iterrows():\n    quote = row['Text']\n    story = generate_story(quote)\n    print(story)\n    print()\n\n# Note: You can customize the story generation logic to create more meaningful stories.\n","metadata":{"execution":{"iopub.status.busy":"2023-11-11T23:21:36.326136Z","iopub.execute_input":"2023-11-11T23:21:36.326601Z","iopub.status.idle":"2023-11-11T23:21:37.164388Z","shell.execute_reply.started":"2023-11-11T23:21:36.326566Z","shell.execute_reply":"2023-11-11T23:21:37.163314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load the dataset\ndataset_path = \"/kaggle/input/cleaneddd/cleaned_quotes.csv\"\ndf = pd.read_csv(dataset_path)\n\n# Load the StableBeluga-13B model directly\nmodel_name = \"stabilityai/StableBeluga-13B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Define a function to generate a short story\ndef generate_story(quote):\n    # Tokenize the quote\n    input_ids = tokenizer.encode(quote, return_tensors=\"pt\", max_length=50, truncation=True)\n    \n    # Generate text based on the quote\n    output = model.generate(input_ids, max_length=3000, num_beams=5, temperature=0.7, top_k=50, top_p=0.95, no_repeat_ngram_size=2, num_return_sequences=1)\n    \n    # Decode the generated text\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    \n    # Create a story incorporating the generated text\n    story = f\"Write a short story based on the inspirational quote:\\n'{quote}'\\n\\n{generated_text}\"\n    \n    return story\n\n# Generate stories for the first row\nfor index, row in df.head(1).iterrows():\n    quote = row['Text']\n    story = generate_story(quote)\n    print(story)\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-11T23:22:48.566304Z","iopub.execute_input":"2023-11-11T23:22:48.566711Z","iopub.status.idle":"2023-11-11T23:35:47.091289Z","shell.execute_reply.started":"2023-11-11T23:22:48.566681Z","shell.execute_reply":"2023-11-11T23:35:47.090487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}