{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n# Generating Inspirational Short Stories with Transformers\n\n## Overview:\nUsing Transformers library to generate inspirational short stories based on a dataset of quotes. The code utilizes the StableBeluga-13B model for text generation, providing both a high-level pipeline approach and a direct model loading method for more control.\n\n## Content:\n\n1. **Setup:**\n   - Installation of the Transformers library.\n   - Initialization of a high-level text generation pipeline.\n   - Loading the StableBeluga-13B model and tokenizer directly.\n\n2. **Random Story Generation:**\n   - Loading a dataset of quotes (e.g., cleaned_quotes.csv).\n   - Defining a function to generate random short stories based on the provided quotes.\n   - Generating and printing short stories for the first row of the dataset.\n\n3. **Improved Story Generation with Transformers Model:**\n   - Utilizing the StableBeluga-13B model for more sophisticated story generation.\n   - Tokenizing the input quote, generating text, and decoding the output.\n   - Creating and printing short stories that incorporate the generated text for the first row of the dataset.\n\nHappy storytelling!\n","metadata":{}},{"cell_type":"code","source":"pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-11T23:12:44.302205Z","iopub.execute_input":"2023-11-11T23:12:44.302695Z","iopub.status.idle":"2023-11-11T23:12:58.910158Z","shell.execute_reply.started":"2023-11-11T23:12:44.302653Z","shell.execute_reply":"2023-11-11T23:12:58.909010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"stabilityai/StableBeluga-13B\")","metadata":{"execution":{"iopub.status.busy":"2023-11-11T23:12:58.911935Z","iopub.execute_input":"2023-11-11T23:12:58.912209Z","iopub.status.idle":"2023-11-11T23:17:01.033204Z","shell.execute_reply.started":"2023-11-11T23:12:58.912184Z","shell.execute_reply":"2023-11-11T23:17:01.032274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"stabilityai/StableBeluga-13B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"stabilityai/StableBeluga-13B\")","metadata":{"execution":{"iopub.status.busy":"2023-11-11T23:17:01.034395Z","iopub.execute_input":"2023-11-11T23:17:01.034921Z","iopub.status.idle":"2023-11-11T23:18:58.370469Z","shell.execute_reply.started":"2023-11-11T23:17:01.034894Z","shell.execute_reply":"2023-11-11T23:18:58.369679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport random\n\n# Load the dataset\ndataset_path = \"/kaggle/input/cleaneddd/cleaned_quotes.csv\"\ndf = pd.read_csv(dataset_path)\n\n# Define a function to generate a short story\ndef generate_story(quote):\n    # You can customize the story generation logic here.\n    # For this example, we'll generate a random story based on the quote.\n    story = f\"Write a short story with minimum 500 words based on the quote:\\n'{quote}'\\n\"\n    \n    return story\n\n# Generate stories for the first 10 rows\nfor index, row in df.head(1).iterrows():\n    quote = row['Text']\n    story = generate_story(quote)\n    print(story)\n    print()\n\n# Note: You can customize the story generation logic to create more meaningful stories.\n","metadata":{"execution":{"iopub.status.busy":"2023-11-11T23:21:36.326136Z","iopub.execute_input":"2023-11-11T23:21:36.326601Z","iopub.status.idle":"2023-11-11T23:21:37.164388Z","shell.execute_reply.started":"2023-11-11T23:21:36.326566Z","shell.execute_reply":"2023-11-11T23:21:37.163314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load the dataset\ndataset_path = \"/kaggle/input/cleaneddd/cleaned_quotes.csv\"\ndf = pd.read_csv(dataset_path)\n\n# Load the StableBeluga-13B model directly\nmodel_name = \"stabilityai/StableBeluga-13B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Define a function to generate a short story\ndef generate_story(quote):\n    # Tokenize the quote\n    input_ids = tokenizer.encode(quote, return_tensors=\"pt\", max_length=50, truncation=True)\n    \n    # Generate text based on the quote\n    output = model.generate(input_ids, max_length=3000, num_beams=5, temperature=0.7, top_k=50, top_p=0.95, no_repeat_ngram_size=2, num_return_sequences=1)\n    \n    # Decode the generated text\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    \n    # Create a story incorporating the generated text\n    story = f\"Write a short story based on the inspirational quote:\\n'{quote}'\\n\\n{generated_text}\"\n    \n    return story\n\n# Generate stories for the first row\nfor index, row in df.head(1).iterrows():\n    quote = row['Text']\n    story = generate_story(quote)\n    print(story)\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-11T23:22:48.566304Z","iopub.execute_input":"2023-11-11T23:22:48.566711Z","iopub.status.idle":"2023-11-11T23:35:47.091289Z","shell.execute_reply.started":"2023-11-11T23:22:48.566681Z","shell.execute_reply":"2023-11-11T23:35:47.090487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}